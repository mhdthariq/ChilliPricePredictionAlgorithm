{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network Modeling for Chili Price Forecasting\n",
    "\n",
    "**Algorithm:** LSTM (Long Short-Term Memory)\n",
    "\n",
    "**Architecture:**\n",
    "- Multivariate approach: ONE model predicts all 5 markets simultaneously\n",
    "- Two models: LSTM without holidays, LSTM with holidays\n",
    "- Input shape: (LOOK_BACK, num_markets) or (LOOK_BACK, num_markets + 1)\n",
    "- Output: Dense(num_markets) predicting all market prices\n",
    "\n",
    "**Prerequisites:** Run `01_data_cleaning_and_eda.ipynb` first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "✓ TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('../models/lstm', exist_ok=True)\n",
    "os.makedirs('../results/metrics', exist_ok=True)\n",
    "\n",
    "print('✓ Libraries imported successfully')\n",
    "print(f'✓ TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (471, 6)\n",
      "Date range: 2024-01-01 00:00:00 to 2025-10-24 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pasar Aksara</th>\n",
       "      <th>Pasar Brayan</th>\n",
       "      <th>Pasar Petisah</th>\n",
       "      <th>Pasar Sukaramai</th>\n",
       "      <th>Pusat Pasar</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>26500.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>32500.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>32500.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>24500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pasar Aksara  Pasar Brayan  Pasar Petisah  Pasar Sukaramai  \\\n",
       "Date                                                                     \n",
       "2024-01-01       30000.0       26500.0        30000.0          30000.0   \n",
       "2024-01-02       35000.0       32500.0        35000.0          38000.0   \n",
       "2024-01-03       35000.0       32500.0        35000.0          38000.0   \n",
       "2024-01-04       30000.0       30000.0        30000.0          38000.0   \n",
       "2024-01-05       30000.0       30000.0        30000.0          30000.0   \n",
       "\n",
       "            Pusat Pasar  is_holiday  \n",
       "Date                                 \n",
       "2024-01-01      27500.0           1  \n",
       "2024-01-02      30000.0           0  \n",
       "2024-01-03      30000.0           0  \n",
       "2024-01-04      30000.0           0  \n",
       "2024-01-05      24500.0           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "df_with_holidays = pd.read_csv('../data/processed/data_with_holidays.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Data loaded: {df_with_holidays.shape}\")\n",
    "print(f\"Date range: {df_with_holidays.index.min()} to {df_with_holidays.index.max()}\")\n",
    "df_with_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markets: 5\n",
      "Look-back window: 30 days\n",
      "Epochs: 50\n",
      "Batch size: 16\n",
      "\n",
      "Training data: 376 days (2024-01-01 00:00:00 to 2025-06-13 00:00:00)\n",
      "Testing data: 95 days (2025-06-16 00:00:00 to 2025-10-24 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "market_columns = ['Pasar Sukaramai', 'Pasar Aksara', 'Pasar Petisah', 'Pusat Pasar', 'Pasar Brayan']\n",
    "TEST_SIZE = 0.2\n",
    "LOOK_BACK = 30  # Use 30 days of history\n",
    "SPLIT_INDEX = int(len(df_with_holidays) * (1 - TEST_SIZE))\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(f\"Markets: {len(market_columns)}\")\n",
    "print(f\"Look-back window: {LOOK_BACK} days\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Split data chronologically\n",
    "train_data = df_with_holidays.iloc[:SPLIT_INDEX]\n",
    "test_data = df_with_holidays.iloc[SPLIT_INDEX:]\n",
    "\n",
    "print(f\"\\nTraining data: {train_data.shape[0]} days ({train_data.index[0]} to {train_data.index[-1]})\")\n",
    "print(f\"Testing data: {test_data.shape[0]} days ({test_data.index[0]} to {test_data.index[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16686b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scalers from data/scalers/...\n",
      "✓ Scalers loaded successfully\n",
      "\n",
      "Scaling data with pre-fitted scalers...\n",
      "Markets-only scaling:\n",
      "  Train shape: (376, 5)\n",
      "  Test shape: (95, 5)\n",
      "\n",
      "Markets + holiday scaling:\n",
      "  Train shape: (376, 6)\n",
      "  Test shape: (95, 6)\n",
      "\n",
      "✓ Data scaled successfully using existing scalers!\n"
     ]
    }
   ],
   "source": [
    "# Helper function for MAPE calculation\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    mask = actual != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    mape = np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100\n",
    "    return min(mape, 999.99)\n",
    "\n",
    "# Load the scalers created in notebook 01\n",
    "print(\"Loading scalers from data/scalers/...\")\n",
    "scaler_markets = joblib.load('../data/scalers/scaler_markets.joblib')\n",
    "scaler_with_features = joblib.load('../data/scalers/scaler_with_features.joblib')\n",
    "print(\"✓ Scalers loaded successfully\")\n",
    "\n",
    "# Scale the data using the loaded scalers\n",
    "print(\"\\nScaling data with pre-fitted scalers...\")\n",
    "\n",
    "# Markets only (for LSTM without holidays)\n",
    "train_markets_scaled = scaler_markets.transform(train_data[market_columns])\n",
    "test_markets_scaled = scaler_markets.transform(test_data[market_columns])\n",
    "\n",
    "print(\"Markets-only scaling:\")\n",
    "print(f\"  Train shape: {train_markets_scaled.shape}\")\n",
    "print(f\"  Test shape: {test_markets_scaled.shape}\")\n",
    "\n",
    "# Markets + holiday feature (for LSTM with holidays)\n",
    "feature_columns = market_columns + ['is_holiday']\n",
    "train_features_scaled = scaler_with_features.transform(train_data[feature_columns])\n",
    "test_features_scaled = scaler_with_features.transform(test_data[feature_columns])\n",
    "\n",
    "print(\"\\nMarkets + holiday scaling:\")\n",
    "print(f\"  Train shape: {train_features_scaled.shape}\")\n",
    "print(f\"  Test shape: {test_features_scaled.shape}\")\n",
    "\n",
    "print(\"\\n✓ Data scaled successfully using existing scalers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b467784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training LSTM Models - Multivariate Approach (ONE model for ALL markets)\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "Model 1: LSTM without holidays (Markets only)\n",
      "==================================================\n",
      "Training data shape: (376, 5)\n",
      "Test data shape: (95, 5)\n",
      "Number of features (markets): 5\n",
      "Train generator samples: 22\n",
      "Test generator samples: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 01:15:03.032690: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/python/3.13.9/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m17,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,501</span> (119.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,501\u001b[0m (119.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,501</span> (119.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,501\u001b[0m (119.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0699 - mae: 0.2041 - val_loss: 0.0559 - val_mae: 0.1826\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0699 - mae: 0.2041 - val_loss: 0.0559 - val_mae: 0.1826\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0315 - mae: 0.1389 - val_loss: 0.0495 - val_mae: 0.1722\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0315 - mae: 0.1389 - val_loss: 0.0495 - val_mae: 0.1722\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0284 - mae: 0.1283 - val_loss: 0.0502 - val_mae: 0.1752\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0284 - mae: 0.1283 - val_loss: 0.0502 - val_mae: 0.1752\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273 - mae: 0.1268 - val_loss: 0.0401 - val_mae: 0.1542\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273 - mae: 0.1268 - val_loss: 0.0401 - val_mae: 0.1542\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0244 - mae: 0.1172 - val_loss: 0.0307 - val_mae: 0.1332\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0244 - mae: 0.1172 - val_loss: 0.0307 - val_mae: 0.1332\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0224 - mae: 0.1153 - val_loss: 0.0332 - val_mae: 0.1418\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0224 - mae: 0.1153 - val_loss: 0.0332 - val_mae: 0.1418\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0209 - mae: 0.1074 - val_loss: 0.0316 - val_mae: 0.1402\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0209 - mae: 0.1074 - val_loss: 0.0316 - val_mae: 0.1402\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0178 - mae: 0.1013 - val_loss: 0.0609 - val_mae: 0.2042\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0178 - mae: 0.1013 - val_loss: 0.0609 - val_mae: 0.2042\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0200 - mae: 0.1104 - val_loss: 0.0317 - val_mae: 0.1435\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0200 - mae: 0.1104 - val_loss: 0.0317 - val_mae: 0.1435\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0185 - mae: 0.1048 - val_loss: 0.0284 - val_mae: 0.1349\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0185 - mae: 0.1048 - val_loss: 0.0284 - val_mae: 0.1349\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0176 - mae: 0.0965 - val_loss: 0.0176 - val_mae: 0.0971\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0176 - mae: 0.0965 - val_loss: 0.0176 - val_mae: 0.0971\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0150 - mae: 0.0937 - val_loss: 0.0484 - val_mae: 0.1819\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0150 - mae: 0.0937 - val_loss: 0.0484 - val_mae: 0.1819\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0164 - mae: 0.0962 - val_loss: 0.0407 - val_mae: 0.1646\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0164 - mae: 0.0962 - val_loss: 0.0407 - val_mae: 0.1646\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0147 - mae: 0.0890 - val_loss: 0.0291 - val_mae: 0.1368\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0147 - mae: 0.0890 - val_loss: 0.0291 - val_mae: 0.1368\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0145 - mae: 0.0890 - val_loss: 0.0338 - val_mae: 0.1474\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0145 - mae: 0.0890 - val_loss: 0.0338 - val_mae: 0.1474\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0129 - mae: 0.0849 - val_loss: 0.0501 - val_mae: 0.1816\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0129 - mae: 0.0849 - val_loss: 0.0501 - val_mae: 0.1816\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0151 - mae: 0.0876 - val_loss: 0.0263 - val_mae: 0.1291\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0151 - mae: 0.0876 - val_loss: 0.0263 - val_mae: 0.1291\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0150 - mae: 0.0892 - val_loss: 0.0358 - val_mae: 0.1510\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0150 - mae: 0.0892 - val_loss: 0.0358 - val_mae: 0.1510\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0130 - mae: 0.0848 - val_loss: 0.0655 - val_mae: 0.2043\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0130 - mae: 0.0848 - val_loss: 0.0655 - val_mae: 0.2043\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0129 - mae: 0.0839 - val_loss: 0.0356 - val_mae: 0.1516\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0129 - mae: 0.0839 - val_loss: 0.0356 - val_mae: 0.1516\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0137 - mae: 0.0854 - val_loss: 0.0351 - val_mae: 0.1502\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0137 - mae: 0.0854 - val_loss: 0.0351 - val_mae: 0.1502\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0143 - mae: 0.0852 - val_loss: 0.0275 - val_mae: 0.1310\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0143 - mae: 0.0852 - val_loss: 0.0275 - val_mae: 0.1310\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0121 - mae: 0.0785 - val_loss: 0.0217 - val_mae: 0.1157\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0121 - mae: 0.0785 - val_loss: 0.0217 - val_mae: 0.1157\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0124 - mae: 0.0808 - val_loss: 0.0225 - val_mae: 0.1176\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0124 - mae: 0.0808 - val_loss: 0.0225 - val_mae: 0.1176\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0115 - mae: 0.0796 - val_loss: 0.0390 - val_mae: 0.1567\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0115 - mae: 0.0796 - val_loss: 0.0390 - val_mae: 0.1567\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0126 - mae: 0.0827 - val_loss: 0.0448 - val_mae: 0.1715\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0126 - mae: 0.0827 - val_loss: 0.0448 - val_mae: 0.1715\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0127 - mae: 0.0839 - val_loss: 0.0270 - val_mae: 0.1303\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0127 - mae: 0.0839 - val_loss: 0.0270 - val_mae: 0.1303\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0111 - mae: 0.0774 - val_loss: 0.0308 - val_mae: 0.1392\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0111 - mae: 0.0774 - val_loss: 0.0308 - val_mae: 0.1392\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0115 - mae: 0.0780 - val_loss: 0.0275 - val_mae: 0.1305\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0115 - mae: 0.0780 - val_loss: 0.0275 - val_mae: 0.1305\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0100 - mae: 0.0722 - val_loss: 0.0392 - val_mae: 0.1572\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0100 - mae: 0.0722 - val_loss: 0.0392 - val_mae: 0.1572\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0115 - mae: 0.0776 - val_loss: 0.0330 - val_mae: 0.1441\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0115 - mae: 0.0776 - val_loss: 0.0330 - val_mae: 0.1441\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0108 - mae: 0.0753 - val_loss: 0.0241 - val_mae: 0.1205\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0108 - mae: 0.0753 - val_loss: 0.0241 - val_mae: 0.1205\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0101 - mae: 0.0718 - val_loss: 0.0390 - val_mae: 0.1566\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0101 - mae: 0.0718 - val_loss: 0.0390 - val_mae: 0.1566\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0094 - mae: 0.0691 - val_loss: 0.0275 - val_mae: 0.1289\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0094 - mae: 0.0691 - val_loss: 0.0275 - val_mae: 0.1289\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098 - mae: 0.0731 - val_loss: 0.0356 - val_mae: 0.1478\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098 - mae: 0.0731 - val_loss: 0.0356 - val_mae: 0.1478\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098 - mae: 0.0697 - val_loss: 0.0191 - val_mae: 0.1044\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098 - mae: 0.0697 - val_loss: 0.0191 - val_mae: 0.1044\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0101 - mae: 0.0732 - val_loss: 0.0271 - val_mae: 0.1285\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0101 - mae: 0.0732 - val_loss: 0.0271 - val_mae: 0.1285\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0105 - mae: 0.0736 - val_loss: 0.0215 - val_mae: 0.1123\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0105 - mae: 0.0736 - val_loss: 0.0215 - val_mae: 0.1123\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0097 - mae: 0.0704 - val_loss: 0.0242 - val_mae: 0.1199\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0097 - mae: 0.0704 - val_loss: 0.0242 - val_mae: 0.1199\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0094 - mae: 0.0680 - val_loss: 0.0221 - val_mae: 0.1131\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0094 - mae: 0.0680 - val_loss: 0.0221 - val_mae: 0.1131\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0088 - mae: 0.0679 - val_loss: 0.0204 - val_mae: 0.1079\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0088 - mae: 0.0679 - val_loss: 0.0204 - val_mae: 0.1079\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0099 - mae: 0.0707 - val_loss: 0.0302 - val_mae: 0.1345\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0099 - mae: 0.0707 - val_loss: 0.0302 - val_mae: 0.1345\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 - mae: 0.0670 - val_loss: 0.0376 - val_mae: 0.1521\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 - mae: 0.0670 - val_loss: 0.0376 - val_mae: 0.1521\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0099 - mae: 0.0707 - val_loss: 0.0319 - val_mae: 0.1386\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0099 - mae: 0.0707 - val_loss: 0.0319 - val_mae: 0.1386\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0091 - mae: 0.0663 - val_loss: 0.0285 - val_mae: 0.1307\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0091 - mae: 0.0663 - val_loss: 0.0285 - val_mae: 0.1307\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0086 - mae: 0.0669 - val_loss: 0.0352 - val_mae: 0.1453\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0086 - mae: 0.0669 - val_loss: 0.0352 - val_mae: 0.1453\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0671 - val_loss: 0.0368 - val_mae: 0.1500\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0671 - val_loss: 0.0368 - val_mae: 0.1500\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0095 - mae: 0.0699 - val_loss: 0.0272 - val_mae: 0.1261\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0095 - mae: 0.0699 - val_loss: 0.0272 - val_mae: 0.1261\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0660 - val_loss: 0.0342 - val_mae: 0.1418\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0660 - val_loss: 0.0342 - val_mae: 0.1418\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0666 - val_loss: 0.0306 - val_mae: 0.1339\n",
      "\n",
      "Making predictions on test set...\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0666 - val_loss: 0.0306 - val_mae: 0.1339\n",
      "\n",
      "Making predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (65, 5)\n",
      "\n",
      "==================================================\n",
      "LSTM (no holidays) - Metrics by Market:\n",
      "==================================================\n",
      "Pasar Sukaramai          : RMSE=13131.67, MAE=10174.77, MAPE= 14.94%\n",
      "Pasar Aksara             : RMSE=10461.79, MAE= 7953.55, MAPE= 12.39%\n",
      "Pasar Petisah            : RMSE=11315.02, MAE= 8650.17, MAPE= 13.02%\n",
      "Pusat Pasar              : RMSE=12523.38, MAE= 9751.93, MAPE= 15.00%\n",
      "Pasar Brayan             : RMSE=12234.50, MAE= 9331.40, MAPE= 13.42%\n",
      "\n",
      "Average RMSE: 11933.27\n",
      "Model saved to: models/lstm/lstm_model_all_markets.h5\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model 2: LSTM with holidays (Markets + Holiday feature)\n",
      "==================================================\n",
      "Training data shape: (376, 6)\n",
      "Test data shape: (95, 6)\n",
      "Number of features (markets + holiday): 6\n",
      "Train generator samples: 22\n",
      "Test generator samples: 5\n",
      "\n",
      "Model architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.13.9/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m18,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m198\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,790</span> (120.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,790\u001b[0m (120.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,790</span> (120.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,790\u001b[0m (120.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model with holidays...\n",
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.1101 - mae: 0.2411 - val_loss: 0.1630 - val_mae: 0.3195\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.1101 - mae: 0.2411 - val_loss: 0.1630 - val_mae: 0.3195\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0571 - mae: 0.1685 - val_loss: 0.0816 - val_mae: 0.2314\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0571 - mae: 0.1685 - val_loss: 0.0816 - val_mae: 0.2314\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0509 - mae: 0.1582 - val_loss: 0.0700 - val_mae: 0.2144\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0509 - mae: 0.1582 - val_loss: 0.0700 - val_mae: 0.2144\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0478 - mae: 0.1527 - val_loss: 0.0543 - val_mae: 0.1845\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0478 - mae: 0.1527 - val_loss: 0.0543 - val_mae: 0.1845\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0470 - mae: 0.1559 - val_loss: 0.0496 - val_mae: 0.1779\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0470 - mae: 0.1559 - val_loss: 0.0496 - val_mae: 0.1779\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0431 - mae: 0.1458 - val_loss: 0.1014 - val_mae: 0.2620\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0431 - mae: 0.1458 - val_loss: 0.1014 - val_mae: 0.2620\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0434 - mae: 0.1405 - val_loss: 0.0398 - val_mae: 0.1565\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0434 - mae: 0.1405 - val_loss: 0.0398 - val_mae: 0.1565\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0435 - mae: 0.1493 - val_loss: 0.0576 - val_mae: 0.1989\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0435 - mae: 0.1493 - val_loss: 0.0576 - val_mae: 0.1989\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0428 - mae: 0.1395 - val_loss: 0.0355 - val_mae: 0.1471\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0428 - mae: 0.1395 - val_loss: 0.0355 - val_mae: 0.1471\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0407 - mae: 0.1388 - val_loss: 0.0400 - val_mae: 0.1599\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0407 - mae: 0.1388 - val_loss: 0.0400 - val_mae: 0.1599\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0381 - mae: 0.1283 - val_loss: 0.0548 - val_mae: 0.1966\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0381 - mae: 0.1283 - val_loss: 0.0548 - val_mae: 0.1966\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0370 - mae: 0.1252 - val_loss: 0.0306 - val_mae: 0.1344\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0370 - mae: 0.1252 - val_loss: 0.0306 - val_mae: 0.1344\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0369 - mae: 0.1269 - val_loss: 0.0380 - val_mae: 0.1590\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0369 - mae: 0.1269 - val_loss: 0.0380 - val_mae: 0.1590\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0361 - mae: 0.1236 - val_loss: 0.0391 - val_mae: 0.1639\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0361 - mae: 0.1236 - val_loss: 0.0391 - val_mae: 0.1639\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0352 - mae: 0.1217 - val_loss: 0.0465 - val_mae: 0.1777\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0352 - mae: 0.1217 - val_loss: 0.0465 - val_mae: 0.1777\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0343 - mae: 0.1212 - val_loss: 0.0464 - val_mae: 0.1789\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0343 - mae: 0.1212 - val_loss: 0.0464 - val_mae: 0.1789\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0349 - mae: 0.1239 - val_loss: 0.0309 - val_mae: 0.1368\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0349 - mae: 0.1239 - val_loss: 0.0309 - val_mae: 0.1368\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0330 - mae: 0.1142 - val_loss: 0.0393 - val_mae: 0.1630\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0330 - mae: 0.1142 - val_loss: 0.0393 - val_mae: 0.1630\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0323 - mae: 0.1161 - val_loss: 0.0360 - val_mae: 0.1533\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0323 - mae: 0.1161 - val_loss: 0.0360 - val_mae: 0.1533\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0323 - mae: 0.1147 - val_loss: 0.0424 - val_mae: 0.1703\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0323 - mae: 0.1147 - val_loss: 0.0424 - val_mae: 0.1703\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0298 - mae: 0.1081 - val_loss: 0.0541 - val_mae: 0.1934\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0298 - mae: 0.1081 - val_loss: 0.0541 - val_mae: 0.1934\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0297 - mae: 0.1123 - val_loss: 0.0270 - val_mae: 0.1286\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0297 - mae: 0.1123 - val_loss: 0.0270 - val_mae: 0.1286\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0286 - mae: 0.1082 - val_loss: 0.0500 - val_mae: 0.1870\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0286 - mae: 0.1082 - val_loss: 0.0500 - val_mae: 0.1870\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0299 - mae: 0.1071 - val_loss: 0.0340 - val_mae: 0.1504\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0299 - mae: 0.1071 - val_loss: 0.0340 - val_mae: 0.1504\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273 - mae: 0.1078 - val_loss: 0.0327 - val_mae: 0.1459\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273 - mae: 0.1078 - val_loss: 0.0327 - val_mae: 0.1459\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0264 - mae: 0.1038 - val_loss: 0.0290 - val_mae: 0.1352\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0264 - mae: 0.1038 - val_loss: 0.0290 - val_mae: 0.1352\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0251 - mae: 0.1040 - val_loss: 0.0295 - val_mae: 0.1393\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0251 - mae: 0.1040 - val_loss: 0.0295 - val_mae: 0.1393\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0250 - mae: 0.1033 - val_loss: 0.0455 - val_mae: 0.1766\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0250 - mae: 0.1033 - val_loss: 0.0455 - val_mae: 0.1766\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0249 - mae: 0.1028 - val_loss: 0.0259 - val_mae: 0.1260\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0249 - mae: 0.1028 - val_loss: 0.0259 - val_mae: 0.1260\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0229 - mae: 0.1003 - val_loss: 0.0319 - val_mae: 0.1452\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0229 - mae: 0.1003 - val_loss: 0.0319 - val_mae: 0.1452\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0224 - mae: 0.0988 - val_loss: 0.0256 - val_mae: 0.1263\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0224 - mae: 0.0988 - val_loss: 0.0256 - val_mae: 0.1263\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0219 - mae: 0.0971 - val_loss: 0.0258 - val_mae: 0.1277\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0219 - mae: 0.0971 - val_loss: 0.0258 - val_mae: 0.1277\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0208 - mae: 0.0941 - val_loss: 0.0444 - val_mae: 0.1687\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0208 - mae: 0.0941 - val_loss: 0.0444 - val_mae: 0.1687\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0215 - mae: 0.0965 - val_loss: 0.0373 - val_mae: 0.1563\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0215 - mae: 0.0965 - val_loss: 0.0373 - val_mae: 0.1563\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0208 - mae: 0.0948 - val_loss: 0.0612 - val_mae: 0.1978\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0208 - mae: 0.0948 - val_loss: 0.0612 - val_mae: 0.1978\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0190 - mae: 0.0921 - val_loss: 0.0504 - val_mae: 0.1810\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0190 - mae: 0.0921 - val_loss: 0.0504 - val_mae: 0.1810\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0206 - mae: 0.0941 - val_loss: 0.0576 - val_mae: 0.1963\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0206 - mae: 0.0941 - val_loss: 0.0576 - val_mae: 0.1963\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0202 - mae: 0.0928 - val_loss: 0.0413 - val_mae: 0.1609\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0202 - mae: 0.0928 - val_loss: 0.0413 - val_mae: 0.1609\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0199 - mae: 0.0929 - val_loss: 0.0359 - val_mae: 0.1304\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0199 - mae: 0.0929 - val_loss: 0.0359 - val_mae: 0.1304\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0194 - mae: 0.0873 - val_loss: 0.0518 - val_mae: 0.1704\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0194 - mae: 0.0873 - val_loss: 0.0518 - val_mae: 0.1704\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195 - mae: 0.0928 - val_loss: 0.0533 - val_mae: 0.1847\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195 - mae: 0.0928 - val_loss: 0.0533 - val_mae: 0.1847\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0198 - mae: 0.0901 - val_loss: 0.0446 - val_mae: 0.1678\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0198 - mae: 0.0901 - val_loss: 0.0446 - val_mae: 0.1678\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0171 - mae: 0.0866 - val_loss: 0.0451 - val_mae: 0.1663\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0171 - mae: 0.0866 - val_loss: 0.0451 - val_mae: 0.1663\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170 - mae: 0.0867 - val_loss: 0.0353 - val_mae: 0.1260\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170 - mae: 0.0867 - val_loss: 0.0353 - val_mae: 0.1260\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0192 - mae: 0.0891 - val_loss: 0.0362 - val_mae: 0.1509\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0192 - mae: 0.0891 - val_loss: 0.0362 - val_mae: 0.1509\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0181 - mae: 0.0876 - val_loss: 0.0393 - val_mae: 0.1573\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0181 - mae: 0.0876 - val_loss: 0.0393 - val_mae: 0.1573\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170 - mae: 0.0834 - val_loss: 0.0429 - val_mae: 0.1580\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170 - mae: 0.0834 - val_loss: 0.0429 - val_mae: 0.1580\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0172 - mae: 0.0833 - val_loss: 0.0433 - val_mae: 0.1618\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0172 - mae: 0.0833 - val_loss: 0.0433 - val_mae: 0.1618\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0168 - mae: 0.0842 - val_loss: 0.0403 - val_mae: 0.1600\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0168 - mae: 0.0842 - val_loss: 0.0403 - val_mae: 0.1600\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0176 - mae: 0.0831 - val_loss: 0.0547 - val_mae: 0.1826\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0176 - mae: 0.0831 - val_loss: 0.0547 - val_mae: 0.1826\n",
      "\n",
      "Making predictions on test set...\n",
      "\n",
      "Making predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (65, 6)\n",
      "\n",
      "==================================================\n",
      "LSTM (with holidays) - Metrics by Market:\n",
      "==================================================\n",
      "Pasar Sukaramai          : RMSE=16995.30, MAE=13799.58, MAPE= 20.70%\n",
      "Pasar Aksara             : RMSE=13057.60, MAE=10539.59, MAPE= 16.47%\n",
      "Pasar Petisah            : RMSE=13765.96, MAE=11157.20, MAPE= 17.15%\n",
      "Pusat Pasar              : RMSE=15210.98, MAE=12388.01, MAPE= 18.90%\n",
      "Pasar Brayan             : RMSE=13459.66, MAE=11216.79, MAPE= 16.87%\n",
      "\n",
      "Average RMSE: 14497.90\n",
      "Model saved to: models/lstm/lstm_holiday_model_all_markets.h5\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LSTM Training Complete!\n",
      "Total models trained: 2 (one without holidays, one with holidays)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training LSTM Models - Multivariate Approach (ONE model for ALL markets)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Model 1: LSTM without holidays (5 features: 5 markets)\n",
    "# ===========================\n",
    "print(\"=\"*50)\n",
    "print(\"Model 1: LSTM without holidays (Markets only)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data - only market columns (5 features)\n",
    "data_no_holiday = train_markets_scaled\n",
    "test_data_nh = test_markets_scaled\n",
    "\n",
    "n_features_nh = data_no_holiday.shape[1]  # 5 markets\n",
    "print(f\"Training data shape: {data_no_holiday.shape}\")\n",
    "print(f\"Test data shape: {test_data_nh.shape}\")\n",
    "print(f\"Number of features (markets): {n_features_nh}\")\n",
    "\n",
    "# Create time series generators\n",
    "train_generator_nh = TimeseriesGenerator(\n",
    "    data_no_holiday, \n",
    "    data_no_holiday,\n",
    "    length=LOOK_BACK,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_generator_nh = TimeseriesGenerator(\n",
    "    test_data_nh,\n",
    "    test_data_nh,\n",
    "    length=LOOK_BACK,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Train generator samples: {len(train_generator_nh)}\")\n",
    "print(f\"Test generator samples: {len(test_generator_nh)}\")\n",
    "\n",
    "# Build LSTM model (input: 5 markets, output: 5 markets)\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(LOOK_BACK, n_features_nh), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(n_features_nh)  # Predict all 5 markets simultaneously\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print(\"\\nModel architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    train_generator_nh,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=test_generator_nh\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "lstm_predictions = []\n",
    "for i in range(len(test_generator_nh)):\n",
    "    X, _ = test_generator_nh[i]\n",
    "    pred = lstm_model.predict(X, verbose=0)\n",
    "    lstm_predictions.extend(pred)\n",
    "\n",
    "lstm_predictions = np.array(lstm_predictions)\n",
    "print(f\"Predictions shape: {lstm_predictions.shape}\")\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "lstm_pred = scaler_markets.inverse_transform(lstm_predictions)\n",
    "actual_test_nh = test_data_nh[LOOK_BACK:LOOK_BACK + len(lstm_pred)]\n",
    "y_test = scaler_markets.inverse_transform(actual_test_nh)\n",
    "\n",
    "# Calculate metrics for each market\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LSTM (no holidays) - Metrics by Market:\")\n",
    "print(\"=\"*50)\n",
    "lstm_rmse_list = []\n",
    "lstm_mae_list = []\n",
    "lstm_mape_list = []\n",
    "\n",
    "for idx, market in enumerate(market_columns):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test[:, idx], lstm_pred[:, idx]))\n",
    "    mae = mean_absolute_error(y_test[:, idx], lstm_pred[:, idx])\n",
    "    mape = calculate_mape(y_test[:, idx], lstm_pred[:, idx])\n",
    "    \n",
    "    lstm_rmse_list.append(rmse)\n",
    "    lstm_mae_list.append(mae)\n",
    "    lstm_mape_list.append(mape)\n",
    "    \n",
    "    print(f\"{market:25s}: RMSE={rmse:8.2f}, MAE={mae:8.2f}, MAPE={mape:6.2f}%\")\n",
    "\n",
    "avg_lstm = np.mean(lstm_rmse_list)\n",
    "print(f\"\\nAverage RMSE: {avg_lstm:.2f}\")\n",
    "\n",
    "# Save the model\n",
    "lstm_model.save('../models/lstm/lstm_model_all_markets.h5')\n",
    "print(\"Model saved to: models/lstm/lstm_model_all_markets.h5\\n\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Model 2: LSTM with holidays (6 features: 5 markets + holiday)\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model 2: LSTM with holidays (Markets + Holiday feature)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data - markets + holiday indicator (6 features)\n",
    "data_with_holiday = train_features_scaled\n",
    "test_data_wh = test_features_scaled\n",
    "\n",
    "n_features_wh = data_with_holiday.shape[1]  # 6 features\n",
    "print(f\"Training data shape: {data_with_holiday.shape}\")\n",
    "print(f\"Test data shape: {test_data_wh.shape}\")\n",
    "print(f\"Number of features (markets + holiday): {n_features_wh}\")\n",
    "\n",
    "# Create time series generators\n",
    "train_generator_wh = TimeseriesGenerator(\n",
    "    data_with_holiday,\n",
    "    data_with_holiday,\n",
    "    length=LOOK_BACK,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_generator_wh = TimeseriesGenerator(\n",
    "    test_data_wh,\n",
    "    test_data_wh,\n",
    "    length=LOOK_BACK,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Train generator samples: {len(train_generator_wh)}\")\n",
    "print(f\"Test generator samples: {len(test_generator_wh)}\")\n",
    "\n",
    "# Build LSTM model with holiday (input: 6 features, output: 6 features)\n",
    "lstm_holiday_model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(LOOK_BACK, n_features_wh), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(n_features_wh)  # Predict all 6 features\n",
    "])\n",
    "\n",
    "lstm_holiday_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print(\"\\nModel architecture:\")\n",
    "lstm_holiday_model.summary()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining LSTM model with holidays...\")\n",
    "history_h = lstm_holiday_model.fit(\n",
    "    train_generator_wh,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=test_generator_wh\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "lstm_holiday_predictions = []\n",
    "for i in range(len(test_generator_wh)):\n",
    "    X, _ = test_generator_wh[i]\n",
    "    pred = lstm_holiday_model.predict(X, verbose=0)\n",
    "    lstm_holiday_predictions.extend(pred)\n",
    "\n",
    "lstm_holiday_predictions = np.array(lstm_holiday_predictions)\n",
    "print(f\"Predictions shape: {lstm_holiday_predictions.shape}\")\n",
    "\n",
    "# Inverse transform - only take the first 5 columns (markets)\n",
    "lstm_holiday_pred_all = scaler_with_features.inverse_transform(lstm_holiday_predictions)\n",
    "lstm_holiday_pred = lstm_holiday_pred_all[:, :5]  # Extract only market predictions\n",
    "\n",
    "actual_test_wh = test_data_wh[LOOK_BACK:LOOK_BACK + len(lstm_holiday_pred)]\n",
    "y_test_h_all = scaler_with_features.inverse_transform(actual_test_wh)\n",
    "y_test_h = y_test_h_all[:, :5]  # Extract only actual market values\n",
    "\n",
    "# Calculate metrics for each market\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LSTM (with holidays) - Metrics by Market:\")\n",
    "print(\"=\"*50)\n",
    "lstm_h_rmse_list = []\n",
    "lstm_h_mae_list = []\n",
    "lstm_h_mape_list = []\n",
    "\n",
    "for idx, market in enumerate(market_columns):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_h[:, idx], lstm_holiday_pred[:, idx]))\n",
    "    mae = mean_absolute_error(y_test_h[:, idx], lstm_holiday_pred[:, idx])\n",
    "    mape = calculate_mape(y_test_h[:, idx], lstm_holiday_pred[:, idx])\n",
    "    \n",
    "    lstm_h_rmse_list.append(rmse)\n",
    "    lstm_h_mae_list.append(mae)\n",
    "    lstm_h_mape_list.append(mape)\n",
    "    \n",
    "    print(f\"{market:25s}: RMSE={rmse:8.2f}, MAE={mae:8.2f}, MAPE={mape:6.2f}%\")\n",
    "\n",
    "avg_lstm_h = np.mean(lstm_h_rmse_list)\n",
    "print(f\"\\nAverage RMSE: {avg_lstm_h:.2f}\")\n",
    "\n",
    "# Save the model\n",
    "lstm_holiday_model.save('../models/lstm/lstm_holiday_model_all_markets.h5')\n",
    "print(\"Model saved to: models/lstm/lstm_holiday_model_all_markets.h5\\n\")\n",
    "\n",
    "# Store results for later comparison\n",
    "lstm_results = {\n",
    "    'predictions_no_holiday': lstm_pred,\n",
    "    'predictions_with_holiday': lstm_holiday_pred,\n",
    "    'actual': y_test,  # Use y_test since both have same actual values\n",
    "    'test_dates': test_data.index[LOOK_BACK:LOOK_BACK + len(lstm_pred)],\n",
    "    'rmse_no_holiday': lstm_rmse_list,\n",
    "    'mae_no_holiday': lstm_mae_list,\n",
    "    'mape_no_holiday': lstm_mape_list,\n",
    "    'rmse_with_holiday': lstm_h_rmse_list,\n",
    "    'mae_with_holiday': lstm_h_mae_list,\n",
    "    'mape_with_holiday': lstm_h_mape_list,\n",
    "    'avg_rmse_no_holiday': avg_lstm,\n",
    "    'avg_rmse_with_holiday': avg_lstm_h\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM Training Complete!\")\n",
    "print(f\"Total models trained: 2 (one without holidays, one with holidays)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LSTM FINAL SUMMARY:\n",
      "======================================================================\n",
      "LSTM (no holidays):\n",
      "  Average RMSE: 11,933.27\n",
      "  Average MAPE: 13.76%\n",
      "\n",
      "LSTM (with holidays):\n",
      "  Average RMSE: 14,497.90\n",
      "  Average MAPE: 18.02%\n",
      "\n",
      "Holiday feature improvement: -21.49%\n",
      "======================================================================\n",
      "\n",
      "✓ LSTM results saved to results/metrics/\n",
      "✓ Average RMSE (no holiday): 11933.27\n",
      "✓ Average RMSE (with holiday): 14497.90\n",
      "✓ Average MAPE (no holiday): 13.76%\n",
      "✓ Average MAPE (with holiday): 18.02%\n"
     ]
    }
   ],
   "source": [
    "# Calculate average MAPE values\n",
    "avg_mape_no_holiday = np.mean(lstm_results['mape_no_holiday'])\n",
    "avg_mape_with_holiday = np.mean(lstm_results['mape_with_holiday'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM FINAL SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"LSTM (no holidays):\")\n",
    "print(f\"  Average RMSE: {lstm_results['avg_rmse_no_holiday']:,.2f}\")\n",
    "print(f\"  Average MAPE: {avg_mape_no_holiday:.2f}%\")\n",
    "print(f\"\\nLSTM (with holidays):\")\n",
    "print(f\"  Average RMSE: {lstm_results['avg_rmse_with_holiday']:,.2f}\")\n",
    "print(f\"  Average MAPE: {avg_mape_with_holiday:.2f}%\")\n",
    "\n",
    "improvement = ((lstm_results['avg_rmse_no_holiday'] - lstm_results['avg_rmse_with_holiday']) / lstm_results['avg_rmse_no_holiday']) * 100\n",
    "print(f\"\\nHoliday feature improvement: {improvement:+.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save LSTM results for inference notebook\n",
    "lstm_summary = {\n",
    "    'algorithm': 'LSTM',\n",
    "    'avg_rmse_no_holiday': lstm_results['avg_rmse_no_holiday'],\n",
    "    'avg_rmse_with_holiday': lstm_results['avg_rmse_with_holiday'],\n",
    "    'avg_mape_no_holiday': avg_mape_no_holiday,\n",
    "    'avg_mape_with_holiday': avg_mape_with_holiday,\n",
    "    'markets': market_columns,\n",
    "    'results': lstm_results\n",
    "}\n",
    "\n",
    "joblib.dump(lstm_summary, '../results/metrics/lstm_summary.pkl')\n",
    "joblib.dump(lstm_results, '../results/metrics/lstm_detailed_results.pkl')\n",
    "\n",
    "print('\\n✓ LSTM results saved to results/metrics/')\n",
    "print(f'✓ Average RMSE (no holiday): {lstm_summary[\"avg_rmse_no_holiday\"]:.2f}')\n",
    "print(f'✓ Average RMSE (with holiday): {lstm_summary[\"avg_rmse_with_holiday\"]:.2f}')\n",
    "print(f'✓ Average MAPE (no holiday): {avg_mape_no_holiday:.2f}%')\n",
    "print(f'✓ Average MAPE (with holiday): {avg_mape_with_holiday:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
